import os
import librosa
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader, random_split
import kagglehub

# Step 1: Extract Mel Spectrogram
def extract_mel_spectrogram(audio_path, n_mels=128, fixed_width=432):
    y, sr = librosa.load(audio_path, sr=None)
    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)

    # Padding or trimming
    if mel_spec_db.shape[1] < fixed_width:
        pad_width = fixed_width - mel_spec_db.shape[1]
        mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant')
    else:
        mel_spec_db = mel_spec_db[:, :fixed_width]

    return torch.tensor(mel_spec_db, dtype=torch.float32)

# Step 2: Create Custom Dataset
class AnimalSoundDataset(Dataset):
    def __init__(self, dataset_path, transform=None):
        self.dataset_path = dataset_path
        self.transform = transform
        self.data = []
        self.labels = []
        
        # Ensure labels are consistently mapped
        self.label_map = {label: idx for idx, label in enumerate(sorted(os.listdir(dataset_path)))}
        
        for label in self.label_map:
            class_path = os.path.join(dataset_path, label)
            if os.path.isdir(class_path):
                print(os.listdir(class_path))
                for file in os.listdir(class_path):
                    if file.endswith(".ogg"):
                        self.data.append(os.path.join(class_path, file))
                        self.labels.append(self.label_map[label])

        if len(self.data) == 0:
            raise ValueError("No audio files found in dataset! Check dataset path.")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        audio_path = self.data[idx]
        label = self.labels[idx]
        mel_spec = extract_mel_spectrogram(audio_path)
        
        if self.transform:
            mel_spec = self.transform(mel_spec)
        
        return mel_spec.unsqueeze(0), label

# Step 3: Load Dataset
transform = transforms.Compose([transforms.Lambda(lambda x: x / x.max())])
dataset_path = os.path.join(kagglehub.dataset_download("ouaraskhelilrafik/tp-02-audio"), "Data/Data")
print(os.listdir(dataset_path))
dataset = AnimalSoundDataset(dataset_path, transform=transform)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Step 4: Define CNN Model
class CNNClassifier(nn.Module):
    def __init__(self, num_classes):
        super(CNNClassifier, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=3)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=7, padding=4)
        self.pool = nn.MaxPool2d(2, 2)
        sample_audio = os.path.join(dataset_path, os.listdir(dataset_path)[0], os.listdir(os.path.join(dataset_path, os.listdir(dataset_path)[0]))[0])
        sample_mel_spec = extract_mel_spectrogram(sample_audio)
        feature_size = 64 * (sample_mel_spec.shape[0] // 4) * (sample_mel_spec.shape[1] // 4)
        self.fc1 = nn.Linear(feature_size, 128)
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(x.shape[0], -1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

'''
Train the model using mel spec and label inputs
'''
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_classes = len(dataset.label_map)
model = CNNClassifier(num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(5):
    model.train()
    running_loss = 0.0
    
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    
    print(f"Epoch {epoch+1}/30, Loss: {running_loss/len(train_loader):.4f}")


'''
Evaluates the model with new sounds

Prints a percentage for test accuracy
'''
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f"Test Accuracy: {100 * correct / total:.2f}%")

'''
Predict a new sound using the model
'''
def predict_animal_sound(audio_path, model):
    model.eval()
    mel_spec = extract_mel_spectrogram(audio_path)
    mel_spec = mel_spec / mel_spec.max()
    mel_spec = mel_spec.unsqueeze(0).unsqueeze(0).to(device)
    
    with torch.no_grad():
        output = model(mel_spec)
        predicted_label = torch.argmax(output).item()
    
    return list(dataset.label_map.keys())[predicted_label]

# # Test Prediction
# predicted_animal = predict_animal_sound("path/to/new/audio.wav", model)
# print(f"Predicted animal: {predicted_animal}")